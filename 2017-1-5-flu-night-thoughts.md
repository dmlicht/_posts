---
layout: post
title: "Flu Night Thoughts"
comments: false
description: ""
categories: rambles
tags: neural-networks xgboost
---

There is something insane about neural networks.
I’m sitting here on my bed at 9:23PM, with the flu, watching the numbers roll down my screen.
Hmm. I can’t get even close to the results I got with xgboost.
What if I change the `learning rate` and let it run longer?
What if I add `decay` for the `learning rate` and an extra `hidden layer`.
What if I let it run for an extra hundred `epochs`.
I know this isn’t how professionals train networks.
They automate training with this param and that param.
They create a master that spins up all these different network configurations to see which is best.
Me, I’m just playing the lottery. But, all of a sudden, my totally poop emoji model can compete.
I mean, it’s still not doing as well as `xgboost`, but maybe I can use it in a second layer ensemble with `xgboost` to push my accuracy over the edge.
It’s like they're just magic. I mean, I understand how it's implemented, but it’s beautiful. I just hope it doesn’t crash my chrome again.
I’ll just minimize the window until its done.