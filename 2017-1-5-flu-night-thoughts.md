---
layout: post
title: "Flu Night Thoughts"
comments: false
description: ""
keywords: "Neural Networks"
---

There is something insane about neural networks. As I’m sitting here on my bed at 9:23PM with the flu watching the numbers roll down my screen. Hmm, I can’t get even close to the results I got with xgboost. What if I change the learning rate and let it run longer? What if I add decay for the learning rate and an extra hidden layer. What if I let it run for an extra hundred epochs.  I know this isn’t how professionals train networks. They automate this param and network tuning process. Me, I’m just playing the lottery. But, all of a sudden, my totally poop emoji model can compete. I mean, it’s still not doing as well as xgboost, but maybe if I second layer ensemble it with xgboost it will push my results over the edge. It’s like their just magic. I mean, I understand how its implemented, but It’s beautiful. I just hope it doesn’t crash my chrome again. I’ll just minimize the window until its done.